{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJfzmQM1zQCH12QGWnKPAG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aleem2/Mastering-ML-using-sklearn/blob/main/Sklearn_template_file.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TL;DR Template for Complete ML Process using SKLearn\n",
        "\n",
        "TL;DR Template for Complete ML Process using SKLearn\n",
        "The aim of this template is to capture the complete ML process in one notebook. This code is part of getting started with Sklearn. This guide aims to add cross-validation neatly in one template file. A user might copy this notebook and bring on their dataset and change the estimator.\n",
        "\n",
        "Hint: How to select an estimator.\n",
        "https://scikit-learn.org/stable/getting_started.html\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PL6ifuMrn6qU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "dfx0ATiAmcC7"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV # GridSearchCV is to tune the hyper parameters.\n",
        "import numpy as np # numpy for data wrangling\n",
        "import pandas as pd # scipy for data wrangling\n",
        "# create a pipeline object\n",
        "pipe = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    LogisticRegression()\n",
        ")\n",
        "# load the iris dataset and split it into train and test sets\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "# fit the whole pipeline\n",
        "pipe.fit(X_train, y_train)\n",
        "# we can now use it like any other estimator\n",
        "acc_score=accuracy_score(pipe.predict(X_test), y_test)\n",
        "\n",
        "result = cross_validate(pipe, X_train, y_train)  # defaults to 5-fold CV"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n Cross validation scores based on 5-fold CV validation = \" + str(result['test_score']))\n",
        "print(\"\\n\\n y_test values = \" + str(y_test))\n",
        "print(\"\\n\\n Predicted values based on X_test data = \" + str(pipe.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7RzHnmhXMM6",
        "outputId": "8c46c3b1-d6d2-44b3-c28a-a2cf3027f84b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " Cross validation scores based on 5-fold CV validation = [0.95652174 0.91304348 0.95454545 0.95454545 0.95454545]\n",
            "\n",
            "\n",
            " y_test values = [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
            " 1]\n",
            "\n",
            "\n",
            " Predicted values based on X_test data = [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
            " 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8SSH60ChWk6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy of the model based on test data = \" + str(acc_score)) # accuracy of test data"
      ],
      "metadata": {
        "id": "Y09oCww5q2j7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "238ad2a2-759b-403b-c3be-248af961ccd4"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model based on test data = 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model selection: choosing estimators and their parameters\n",
        "Scikit exposes various methods which help us select appropriate hyper parameters such as regularization C, solvers and so on. These parameters are specific for different models. Hence a proper approach needs to be adopted. The code below broadly does it in four steps. Firstly, listing the hyperparameters using the get_params() method. Secondly, referring to Scikit documentation to identify the appropriate parameters and their ranges. Thirdly, use the GridSearchCV() method to find the appropriate values based on the training data. Finally, using cross_validate() method to confirm and finalize the ML process."
      ],
      "metadata": {
        "id": "VfFu6y8iYJSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# So what parameters should I be considering?\n",
        "pipe.get_params()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MR3G6HUJQR2h",
        "outputId": "45543396-a19d-4df3-cf72-7fe8330731fb"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'memory': None,\n",
              " 'steps': [('standardscaler', StandardScaler()),\n",
              "  ('logisticregression', LogisticRegression())],\n",
              " 'verbose': False,\n",
              " 'standardscaler': StandardScaler(),\n",
              " 'logisticregression': LogisticRegression(),\n",
              " 'standardscaler__copy': True,\n",
              " 'standardscaler__with_mean': True,\n",
              " 'standardscaler__with_std': True,\n",
              " 'logisticregression__C': 1.0,\n",
              " 'logisticregression__class_weight': None,\n",
              " 'logisticregression__dual': False,\n",
              " 'logisticregression__fit_intercept': True,\n",
              " 'logisticregression__intercept_scaling': 1,\n",
              " 'logisticregression__l1_ratio': None,\n",
              " 'logisticregression__max_iter': 100,\n",
              " 'logisticregression__multi_class': 'auto',\n",
              " 'logisticregression__n_jobs': None,\n",
              " 'logisticregression__penalty': 'l2',\n",
              " 'logisticregression__random_state': None,\n",
              " 'logisticregression__solver': 'lbfgs',\n",
              " 'logisticregression__tol': 0.0001,\n",
              " 'logisticregression__verbose': 0,\n",
              " 'logisticregression__warm_start': False}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To optimize the hyper parameters, this code uses GridSearchCV method. The code captures the process."
      ],
      "metadata": {
        "id": "GDZDj4fNyET4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#grid search allows for hyper parameter optimisation\n",
        "# Target parameter to optimise can be selected as\n",
        "#'logisticregression__solver': 'lbfgs'\n",
        "#'logisticregression__penalty': 'l2'\n",
        "#'logisticregression__max_iter': 100\n",
        "#'logisticregression__C': 1.0\n",
        "\n",
        "pipe=LogisticRegression()\n",
        "Cs = np.linspace(0.1, 0.9, 10)\n",
        "print(\"\\n \\n Cs values being fed in the search = \" + str(Cs))\n",
        "clf = GridSearchCV(estimator=pipe, param_grid=dict(C=Cs,solver=['lbfgs','liblinear', 'newton-cg', 'newton-cholesky','sag','saga'], max_iter=[2000]))\n",
        "clf.fit(X_train, y_train)\n",
        "print(\"\\n\\n Verifying the accuracy of the final model\" + str(clf.score(X_test, y_test)))\n",
        "print(\"\\n\\n Best estimator final values = \" + str(clf.best_estimator_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUfmy4lPZeX1",
        "outputId": "60363fb1-0a76-4c69-bb10-260952e496a3"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " Cs values being fed in the search = [0.1        0.18888889 0.27777778 0.36666667 0.45555556 0.54444444\n",
            " 0.63333333 0.72222222 0.81111111 0.9       ]\n",
            "\n",
            "\n",
            " Verifying the accuracy of the final model0.9736842105263158\n",
            "\n",
            "\n",
            " Best estimator final values = LogisticRegression(C=0.8111111111111111, max_iter=2000, solver='saga')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Findings going through the whole process are as follows. Selecting appropriate regularization value C and the max_iter value were interrelated. A lower value of max_iter would mean the model would not converge. While some solvers performed better than others. Next based on the model hyper parameters, the model can be validated against the training data to complete the training process."
      ],
      "metadata": {
        "id": "Y0ENcf_JlfmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    LogisticRegression(solver='saga',C=0.8111,max_iter=2000)\n",
        ")\n",
        "# load the iris dataset and split it into train and test sets\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "# fit the whole pipeline\n",
        "pipe.fit(X_train, y_train)\n",
        "# we can now use it like any other estimator\n",
        "acc_score=accuracy_score(pipe.predict(X_test), y_test)\n",
        "\n",
        "result = cross_validate(pipe, X_train, y_train)  # defaults to 5-fold CV\n",
        "\n",
        "print(\"\\n\\n Model Accuracy = \" + str(acc_score)) # accuracy of model on the test data\n",
        "print(\"\\n\\n Cross validation scores based on 5-fold CV validation = \" + str(acc_score))\n",
        "print(\"\\n\\n y_test values = \" + str(y_test))\n",
        "print(\"\\n\\n Predicted values based on X_test data = \" + str(pipe.predict(X_test)))"
      ],
      "metadata": {
        "id": "tQVXVUOxqpiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c45eeee3-31f0-49e1-b992-f47de8997da4"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " Model Accuracy = 0.9736842105263158\n",
            "\n",
            "\n",
            " Cross validation scores based on 5-fold CV validation = 0.9736842105263158\n",
            "\n",
            "\n",
            " y_test values = [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
            " 1]\n",
            "\n",
            "\n",
            " Predicted values based on X_test data = [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
            " 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observations\n",
        "\n",
        "\n",
        "1.  The finding was that lbfgs, sag, newton-cg and saga gave exactly the same values, while others were underperforming.\n",
        "2.  The final accuracy on the test set in identifying if a passenger survived or not was 0.9736 by using logistic regression with Saga solver. The regularization value C was 0.811 and max_iter was 2000.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y8iintldnVPz"
      }
    }
  ]
}